# Sorting + Aggregations

### High level Query Plan

The operators are arranged in a tree

Data flows from the leaves of the tree up to towards the root

The output of the root node is the result of the query



### Disk Oriented DBMS

We cannot assume that a table fits entirely in memory

We also cannot assume that query result fit in memory

We are going to rely on buffer pool to implement algorithms that need to spill to disk

We are also going to prefer algorithms that maximize the amount of sequential I/O



## External Merge Sort

**Why do we need to sort?**  - Relational model/SQL is unsorted

Queries may request that tuples are sorted in a specific way `ORDER BY`

Even if a query does not specify an order, we may still want to sort to do other things:

+ Trivial to support duplicate elimination
+ Bulk loading sorted tuples into B+ Tree index is faster
+ Aggregations `GROUP BY`



If data fits in memory, then we can use a standard sorting algorithm like quick-sort

If data does not fit the memory, then we need to use a technique that is aware of the cost of reading and writing disk pages



### Divide - and - Conquer 

Splits data into separate **runs**, sorts them individually, and then combines them into longer sorted runs

#### Phase #1 - Sorting

+ Sort chunk of data that fit in memory and then write back the sorted chunks to a file on disk

#### Phase #2 - Merging

+ Combine sorted runs into larger chunks



#### Sorted run

A run is a list of key/value pairs

**Key:** The attribute to compare to compute the sort order

**Value: **Two choices

+ Tuple (early materialization)
+ Record ID (late materialization)



#### 2 - Way External Merge Sort

2 is the number of runs that we are going to merge into a new run for each pages



Data is broken up into **N** pages - 磁盘中有 N 块数据

The DBMS has a finite number of **B** buffer pool pages to hold input and output data - 内存中能够一次存储 B 个数据



#### Pass #0

+ Read all B pages of the table into memory
+ Sort pages into runs and write them back to disk

#### Pass #1, 2, 3

+ Recursively merge pairs of runs into runs twice as long
+ 将两组排好序的pages合并并进行排序
+ Use three buffer pages (2 for input pages, 1 for output)
+ 合并阶段不占用buffer pool ?



Number of passes = 1 + log2N

Total I/O cost = 2N * (# of passes)



This algorithm only requires three buffer pool pages to perform the sorting B = 3



#### Double Buffering Optimization

Prefetch the next run in the background and store it in a second buffer while the system is processing the current run

+ Reduces the wait time for I/O requests at each step by continuously utilizing the disk
+ Number of passes  = 1 + ceiling[log(b - 1) ceiling(N / B)]



#### Example

Determine how many passes it takes to sort 108 pages with 5 buffer pool pages: N = 108, B = 5

+ **Pass #0:**  ceiling[108 / 5] = 22 sorted runs of 5 pages each with last run is only 3 pages
+ **Pass #1: ** ceiling[22 / 4] = 6 sorted runs of 20 pages each with last run is only 8 pages
+ **Pass #2: ** ceiling[6 / 4] = 2 sorted runs, first one has 80 pages and second one has 28 pages
+ **Pass #3: ** Sorted file of 108 pages

1 + ceiling( log (b - 1) ceiling(N / B) ) = 1 + ceiling( log (5 - 1) ceiling(108 / 5) ) = 4 passes



### Using B+ Trees for sorting

If the table that must be sorted already has a B+ Tree index on the sort attributes, then we can use that to accelerate sorting

Retrieve tuples in desired sort order by simply traversing the leaf pages of the tree



**Clustered B+ Tree: ** Traverse to the left-most leaf page, and then retrieve tuples from all leaf pages

Always better than external sorting because there is no computational cost, and all disk access is sequential



**Un-clustered B+ Tree: ** Chase each pointer to the page that contains the data

This is almost always a bad idea. In general, one I/O per data record



## Aggregations

Collapse values for a single attribute from multiple tuples into a single scalar value



Two implementation choices:

+ Sorting
+ Hashing



What if we do not need the data to be ordered?

+ Forming groups in `GROUP BY` (no ordering)
+ Removing duplicates in `DISTINCT` (no ordering)



Hashing is a better alternative in this scenario

+ Only need to remove duplicates, no need for ordering
+ Can be computationally cheaper than sorting



### Hashing Aggregate

If the DBMS must spill data to disk, then we need to be smarter

**Phase #1: ** Partition

+ Divide tuples into buckets based on hash key
+ Write them out to disk when they get full

**Phase #2: ** Re-Hash

+ Build in - memory hash table for each partition and compute the aggregation











